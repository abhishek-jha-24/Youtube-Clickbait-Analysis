# -*- coding: utf-8 -*-
"""Untitled46.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kw4lNbxK4g15yqi4owR00LihfGrxP_wb
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import files
uploaded = files.upload()
import re
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns
import string
import nltk
import warnings 
warnings.filterwarnings("ignore", category=DeprecationWarning)

# %matplotlib inline

import io

train = pd.read_csv(io.StringIO(uploaded['dataset.csv'].decode('utf-8')), na_filter=False)
print(type(train))
train_original=train.copy()
print(train)



combine = train.copy()
print(combine)

combine['Tidy_Comments'] = combine['comment']
print(combine)


combine['Tidy_Comments'] = combine['Tidy_Comments'].str.replace("[^a-zA-Z#]", " ")

combine.head(10)

# remove wordss less than 3 length
combine['Tidy_Comments'] = combine['Tidy_Comments'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>=2]))

combine.head(10)

tokenized_tweet = combine['Tidy_Comments'].apply(lambda x: x.split())

tokenized_tweet.head()

from nltk import PorterStemmer

ps = PorterStemmer()

tokenized_tweet = tokenized_tweet.apply(lambda x: [ps.stem(i) for i in x])

tokenized_tweet.head()

for i in range(len(tokenized_tweet)):
    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])

combine['Tidy_Comments'] = tokenized_tweet
combine.head()

from wordcloud import WordCloud,ImageColorGenerator
from PIL import Image
import urllib
import requests

all_words_positive = ' '.join(text for text in combine['Tidy_Comments'][combine['label']==1])


all_words_positive


from collections import Counter

split_it = all_words_positive.split()
  
# Pass the split_it list to instance of Counter class.
Counter = Counter(split_it)
  
# most_common() produces k frequently encountered
# input values and their respective counts.
most_occur = Counter.most_common(50)
  
print(most_occur)
all_words_positive

# combining the image with the dataset
Mask = np.array(Image.open(requests.get('https://upload.wikimedia.org/wikipedia/commons/4/47/PNG_transparency_demonstration_1.png', stream=True).raw))

# We use the ImageColorGenerator library from Wordcloud 
# Here we take the color of the image and impose it over our wordcloud
image_colors = ImageColorGenerator(Mask)

# Now we use the WordCloud function from the wordcloud library 
wc = WordCloud(background_color='black', height=1500, width=4000,mask=Mask).generate(all_words_positive)

# Size of the image generated 
plt.figure(figsize=(30,60))

plt.imshow(wc.recolor(color_func=image_colors),interpolation="hamming")

plt.axis('off')
plt.show()

from sklearn.feature_extraction.text import CountVectorizer

bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')

# bag-of-words feature matrix
bow = bow_vectorizer.fit_transform(combine['Tidy_Comments'])

df_bow = pd.DataFrame(bow.todense())

df_bow

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

X = df_bow
X.columns = bow_vectorizer.get_feature_names_out()
y = combine['label']

print(X)

from sklearn.feature_selection import SelectKBest, chi2
t = SelectKBest(chi2, k=100)

x_new = t.fit_transform(X, y)
feature_names_k = t.get_feature_names_out()
feature_names_k

df_scores = pd.DataFrame(t.scores_)
df_columns = pd.DataFrame(bow_vectorizer.get_feature_names_out())

feature_scores = pd.concat([df_columns, df_scores], axis=1)
feature_scores.columns = ["specs", "score"]

print(type(feature_scores))

final_df = feature_scores.sort_values(by=['score'], ascending=False)
final_df[:100]['specs']

len(t.get_feature_names_out())

x_new = pd.DataFrame(x_new)
x_new.columns = feature_names_k
x_new

from sklearn.feature_selection import VarianceThreshold
selector = VarianceThreshold(0.003)
x_var = selector.fit_transform(x_new)
x_var.shape
var_features = selector.get_feature_names_out()
print(len(selector.get_feature_names_out()))



bow = x_var

x_new.shape

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf=TfidfVectorizer(max_df=0.90, min_df=2,max_features=1000,stop_words='english')

tfidf_matrix=tfidf.fit_transform(combine['Tidy_Comments'])

df_tfidf = pd.DataFrame(tfidf_matrix.todense())

df_tfidf

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

X_tf = df_tfidf
y_tf = combine['label']

t_tf = SelectKBest(chi2, k=100)
x_new_tf = t_tf.fit_transform(X_tf, y_tf)
x_new_tf

selector_tf = VarianceThreshold(0.004)
x_var_tf = selector_tf.fit_transform(x_new_tf)
x_var_tf.shape

train_bow = x_var_tf[:31962]
train_bow

train_tfidf_matrix = tfidf_matrix[:31962]

train_tfidf_matrix.todense()

from sklearn.model_selection import train_test_split

x_train_bow, x_valid_bow, y_train_bow, y_valid_bow = train_test_split(train_bow,train['label'],test_size=0.3,random_state=2)

x_train_tfidf, x_valid_tfidf, y_train_tfidf, y_valid_tfidf = train_test_split(train_tfidf_matrix,train['label'],test_size=0.3,random_state=17)

from sklearn.metrics import f1_score

from sklearn.linear_model import LogisticRegression
Log_Reg = LogisticRegression(random_state=0,solver='lbfgs')



Log_Reg.fit(x_train_bow,y_train_bow)

prediction_bow = Log_Reg.predict_proba(x_valid_bow)

prediction_bow

from sklearn.metrics import precision_score
# if prediction is greater than or equal to 0.3 than 1 else 0
# Where 0 is for positive sentiment tweets and 1 for negative sentiment tweets
prediction_int = prediction_bow[:,1]>=0.25
print(prediction_int)
# converting the results to integer type
prediction_int = prediction_int.astype(np.int)
print(prediction_int)
from sklearn.metrics import accuracy_score
# calculating f1 score
log_bow = accuracy_score(y_valid_bow, prediction_int)

print(log_bow)
ans = precision_score(y_valid_bow, prediction_int, average='binary', pos_label=0)
print(ans)

Log_Reg.fit(x_train_tfidf,y_train_tfidf)

prediction_tfidf = Log_Reg.predict_proba(x_valid_tfidf)

prediction_tfidf

# if prediction is greater than or equal to 0.3 than 1 else 0
# Where 0 is for positive sentiment tweets and 1 for negative sentiment tweets
prediction_int = prediction_tfidf[:,1]>=0.3

prediction_int = prediction_int.astype(np.int)
prediction_int

# calculating f1 score
log_tfidf = accuracy_score(y_valid_tfidf, prediction_int)

print(log_tfidf)

ans = precision_score(y_valid_tfidf, prediction_int, average='binary', pos_label=0)
print(ans)

from xgboost import XGBClassifier

model_bow = XGBClassifier(random_state=22,learning_rate=0.9)

model_bow.fit(x_train_bow, y_train_bow)

xgb = model_bow.predict_proba(x_valid_bow)

xgb

# if prediction is greater than or equal to 0.3 than 1 else 0
# Where 0 is for positive sentiment tweets and 1 for negative sentiment tweets
xgb = xgb[:,1]>=0.3

# converting the results to integer type
xgb_int=xgb.astype(np.int)

# calculating f1 score
xgb_bow=f1_score(y_valid_bow,xgb_int)

xgb_bow

model_tfidf = XGBClassifier(random_state=29,learning_rate=0.7)

model_tfidf.fit(x_train_tfidf, y_train_tfidf)

xgb_tfidf=model_tfidf.predict_proba(x_valid_tfidf)

xgb_tfidf

# if prediction is greater than or equal to 0.3 than 1 else 0
# Where 0 is for positive sentiment tweets and 1 for negative sentiment tweets
xgb_tfidf=xgb_tfidf[:,1]>=0.3

# converting the results to integer type
xgb_int_tfidf=xgb_tfidf.astype(np.int)

# calculating f1 score
score=accuracy_score(y_valid_tfidf,xgb_int_tfidf)

score

log_bow = accuracy_score(y_valid_tfidf,xgb_int_tfidf)

print(log_bow)
ans = precision_score(y_valid_tfidf,xgb_int_tfidf, average='binary', pos_label=0)
print(ans)

from sklearn.tree import DecisionTreeClassifier
dct = DecisionTreeClassifier(criterion='entropy', random_state=1)

dct.fit(x_train_bow,y_train_bow)

dct_bow = dct.predict_proba(x_valid_bow)

dct_bow

# if prediction is greater than or equal to 0.3 than 1 else 0
# Where 0 is for positive sentiment tweets and 1 for negative sentiment tweets
dct_bow=dct_bow[:,1]>=0.3

# converting the results to integer type
dct_int_bow=dct_bow.astype(np.int)

# calculating f1 score
dct_score_bow=accuracy_score(y_valid_bow,dct_int_bow)

dct_score_bow

log_bow = accuracy_score(y_valid_bow,dct_int_bow)

print(log_bow)
ans = precision_score(y_valid_bow,dct_int_bow, average='binary', pos_label=0)
print(ans)

dct.fit(x_train_tfidf,y_train_tfidf)

dct_tfidf = dct.predict_proba(x_valid_tfidf)

dct_tfidf

# if prediction is greater than or equal to 0.3 than 1 else 0
# Where 0 is for positive sentiment tweets and 1 for negative sentiment tweets
dct_tfidf=dct_tfidf[:,1]>=0.3

# converting the results to integer type
dct_int_tfidf=dct_tfidf.astype(np.int)

# calculating f1 score
dct_score_tfidf=accuracy_score(y_valid_tfidf,dct_int_tfidf)

dct_score_tfidf

n = input()
clickbait = 0
fair = 0
l = []




from googleapiclient.discovery import build
  
api_key = 'AIzaSyB8RLyfOy62Y33JKvcDoVRARQYXu7-DH5k'
import re
import pandas as pd



name = 1



from html import unescape
import re, string, unicodedata

import urllib.parse as urlparse


def remove_URL(sample):
    """Remove URLs from a sample string"""
    return re.sub(r"http\S+", "", sample)


def video_comments(url):
    # empty list for storing reply
    replies = []
  
    # creating youtube resource object
    youtube = build('youtube', 'v3',
                    developerKey=api_key)

    # retrieve youtube video results
    video_response=youtube.commentThreads().list(
    part='snippet,replies',
    videoId=url
    ).execute()

    # iterate video response
    
        
        # extracting required info
        # from each result object 
    for item in video_response['items']:
        
        # Extracting comments
        comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
          
        # counting number of reply of comment
        replycount = item['snippet']['totalReplyCount']

        # if reply is there
        if replycount>0:
            
            # iterate through all reply
            for reply in item['replies']['comments']:
                
                # Extract reply
                reply = reply['snippet']['textDisplay']
                  
                # Store reply is list
                replies.append(reply)
        comment = remove_URL(comment)
     
        # print comment with list of reply
        comment = (unescape(comment)).replace("<br>", " ")
        comment = comment.replace("\n", " ")
        comment = comment.replace(",", " ")
        comment = comment.replace("@", " ")
        comment = comment.replace("<b>", " ")
        comment = comment.replace("</b>", " ")
        comment = comment.replace('<a href="', " ")
        comment = comment.replace('</a>', " ")
        comment = re.sub(' +', ' ', comment)
        try:
            l.append(comment)
        except Exception as e:
            print(e)
        for resp in replies:

            resp = remove_URL(resp)
            # print comment with list of replyprint(resp, replies, end = '\n\n')
           
            resp = (unescape(resp)).replace("<br>", " ")
            resp = resp.replace("<b>", " ")
            resp = resp.replace("</b>", " ")

            resp = resp.replace("@", " ")
            resp = resp.replace(",", " ")
            resp = resp.replace('<a href="', " ")
            resp = resp.replace('</a>', " ")
            resp = re.sub(' +', ' ', resp)
            try:
                l.append(comment)
            except Exception as e:
                print(e)
        # empty reply list
        replies = []

video_comments(n)
print(l)

from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV

training_data, testing_data = train_test_split(combine, test_size=0.3, random_state=2)

estimators = [('vectorize', TfidfVectorizer()),
              ('reduce_dimension', SelectKBest(chi2)), 
              ('var', VarianceThreshold()),
             ('classifier', LogisticRegression())
             ]
pipe = Pipeline(estimators)


print(len(training_data))

filter = training_data["Tidy_Comments"] != ""
training_data = training_data[filter]
print(len(training_data))

param_grid = dict(vectorize__max_df=[0.99], 
                  vectorize__min_df=[3], 
                  vectorize__max_features=[2000], 
                  vectorize__stop_words=['english'],
                  reduce_dimension__k=[880],
                  var__threshold = [0.0002],
                  classifier__random_state=[0],
                  classifier__solver=['lbfgs'])

grid_search = GridSearchCV(pipe, param_grid=param_grid, cv = 5)
grid_search.fit(training_data['Tidy_Comments']
              , training_data['label'])

print("Test data: ")
print(testing_data['label'])

predicted = grid_search.predict_proba(testing_data['Tidy_Comments'])
print(" \n Prediction Results: ")
print(predicted[:100])




print(len(predicted))
y = [x[0] for x in predicted]
y.sort()
print(y)
print(len(list(set(y))))


predicted = predicted[:,1]>=0.4

print(precision_score(testing_data['label'], predicted, average='binary', pos_label=0))

def pre_process(comment):
  comment = (unescape(comment)).replace("<br>", " ")
  comment = comment.replace("\n", " ")
  comment = comment.replace(",", " ")
  comment = comment.replace("@", " ")
  comment = comment.replace("?", " ")
  comment = comment.replace(".", " ")
  comment = comment.replace("!", " ")
  comment = comment.replace("<b>", " ")
  comment = comment.replace("</b>", " ")
  comment = comment.replace('<a href="', " ")
  comment = comment.replace('</a>', " ")
  comment = comment.replace("[^a-zA-Z#]", " ")
  comment = comment.split()
  
  pre_comment = [i for i in comment if len(i) > 2]
  pre_comment = [ps.stem(i) for i in pre_comment]
  return ' '.join(pre_comment)
y = [pre_process(x) for x in l if len(x) > 3]
print(y)
ok = grid_search.predict_proba(y)
print(ok)
ok = ok[:,1]>=0.4
ok = ok.astype(np.int)
print(l[0], y[0])
print(ok)

